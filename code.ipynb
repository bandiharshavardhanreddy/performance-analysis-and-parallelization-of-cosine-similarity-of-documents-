{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel code\n",
    "def convert_to_lower(x,q1,i,lock):\n",
    "  list1 = x.split(' ')\n",
    "  list3=[]\n",
    "  list3.append(i)\n",
    "  list1 = list3+list1\n",
    "  for i in range(1,len(list1)):\n",
    "    list1[i] = list1[i].lower()\n",
    "  list2 = list1\n",
    "  lock.acquire()\n",
    "  q1.put(list2)\n",
    "  lock.release()\n",
    "def removal_stop_words(lower_case_1,stop_words,q2,num,lock):\n",
    "  set1 = set(lower_case_1)\n",
    "  intersect_words = list(set1.intersection(set(stop_words)))\n",
    "  l_c_f = lower_case_1\n",
    "  for x in intersect_words:\n",
    "    j = 0\n",
    "    while j < len(l_c_f):\n",
    "      if (l_c_f[j] == x):\n",
    "        l_c_f.remove(x)\n",
    "      else:\n",
    "        j += 1\n",
    "  list3=[]\n",
    "  list3.append(num)\n",
    "  l_c_f = list3 + l_c_f\n",
    "  lock.acquire()\n",
    "  q2.put(l_c_f)\n",
    "  lock.release()\n",
    "def find_tf_ntf(lower_case_file,lower_set_file,q3,q4,num4,num5,lock):\n",
    "  list1 = lower_case_file\n",
    "  list2 = lower_set_file\n",
    "  r = len(lower_case_file)\n",
    "  list_count = []\n",
    "  ntf_count = []\n",
    "  for x in list2:\n",
    "    num = list1.count(x)\n",
    "    list_count.append(num)\n",
    "    ntf_count.append(num / r)\n",
    "  list_count=[num4]+list_count\n",
    "  ntf_count = [num5]+ntf_count\n",
    "  lock.acquire()\n",
    "  q3.put(list_count)\n",
    "  q4.put(ntf_count)\n",
    "  lock.release()\n",
    "def idf_inc_query(x,lower_set_file,q6,q7,lock):\n",
    "  len1 = len(lower_set_file)\n",
    "  count1 = 0\n",
    "  for i in range(0, len1):\n",
    "    list1 = lower_set_file[i]\n",
    "    list1=list1[1:]\n",
    "    for j in range(0, len(list1)):\n",
    "      if (list1[j] == x):\n",
    "        count1 += 1\n",
    "        break\n",
    "  lock.acquire()\n",
    "  q6.put(x)\n",
    "  q7.put(1 + math.log2(len1 / count1))\n",
    "  lock.release()\n",
    "  len1 = len(lower_set_file)\n",
    "  count1 = 0\n",
    "  for i in range(0, len1):\n",
    "    list1 = lower_set_file[i]\n",
    "    list1=list1[1:]\n",
    "    for j in range(0, len(list1)):\n",
    "      if (list1[j] == x):\n",
    "        count1 += 1\n",
    "        break\n",
    "  if(count1==0):\n",
    "    count1=1\n",
    "  lock.acquire()\n",
    "  q8.put(x)\n",
    "  q9.put(1 + math.log2(len1 / count1))\n",
    "  lock.release()\n",
    "\n",
    "def find_query_docs_tf_idf(list1,list5,query_list,q3,num,lock):\n",
    "  list2=[]\n",
    "  for x in range(0,len(query_list)):\n",
    "    flag=0\n",
    "    for j in range(0,len(list1)):\n",
    "      if(query_list[x]==list1[j]):\n",
    "        list2.append(list5[j])\n",
    "        flag=1\n",
    "        break\n",
    "    if(flag==0):\n",
    "      list2.append(0)\n",
    "  list2 = [num]+list2\n",
    "  lock.release()\n",
    "\n",
    "def find_doc_tf_dot(list1,query_idf1,q11,q12,num10,lock):\n",
    "  time.sleep(1)\n",
    "  list2 = []\n",
    "  sum1 = 0\n",
    "  for j in range(len(list1)):\n",
    "    list2.append(query_idf1[j] * list1[j])\n",
    "    num3 = query_idf1[j] * list1[j]\n",
    "    sum1 = sum1 + (num3) ** 2\n",
    "  list2 = [num10]+list2\n",
    "  list3=[]\n",
    "  list3.append(math.sqrt(sum1))\n",
    "  list3 =[num10] + list3\n",
    "  lock.acquire()\n",
    "  q11.put(list2)\n",
    "  q12.put(list3)\n",
    "  lock.release()\n",
    "def find_sim(list1,docs_d_p,query_tf_mul_idf,query_dot_prod,num15,q14,lock):\n",
    "  time.sleep(1)\n",
    "  if (docs_d_p == 0.0):\n",
    "    list19=[num15+1,0]\n",
    "    q14.put(list19)\n",
    "    print('Cosine Similarity of Document no ' + str(num15+1) + ' is: 0')\n",
    "  else:\n",
    "    sum1 = 0\n",
    "    for j in range(0, len(query_tf_mul_idf)):\n",
    "      sum1 = sum1 + (query_tf_mul_idf[j] * list1[j])\n",
    "    cos_sim = ((sum1) / (docs_d_p * query_dot_prod))\n",
    "    list19 = [num15+1, cos_sim]\n",
    "    lock.acquire()\n",
    "    q14.put(list19)\n",
    "    lock.release()\n",
    "if __name__ ==\"__main__\":\n",
    "    start = time.perf_counter()\n",
    "    stop_words = [\"\", \";\", \".\\n\", \"i\", \",\", \".\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\"yours\", \"yourself\", \"yourselves\",\"on\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\",\"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\",\"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\",\"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    query = \"impact of demonetization on banks in india\"\n",
    "    main_file = []\n",
    "    file=open('doc.txt','r')\n",
    "    temp=file.read()\n",
    "    main_file.append(temp)\n",
    "    file=open('doc2.txt','r')\n",
    "    temp=file.read()\n",
    "    main_file.append(temp)\n",
    "    file=open('doc3.txt','r')\n",
    "    temp=file.read()\n",
    "    main_file.append(temp)\n",
    "    file=open('doc4.txt','r')\n",
    "    temp=file.read()\n",
    "    main_file.append(temp)\n",
    "    file=open('doc5.txt','r')\n",
    "    temp=file.read()\n",
    "    main_file.append(temp)\n",
    "    main_file.append(query)\n",
    "\n",
    "q1 = mp.Queue()\n",
    "processes = []\n",
    "lock =mp.Lock()\n",
    "for i in range(0,len(main_file)):\n",
    "  p=mp.Process(target=convert_to_lower, args=(main_file[i],q1,i,lock))\n",
    "  processes.append(p)\n",
    "for i in range(0,len(main_file)):\n",
    "  processes[i].start()\n",
    "for i in range(0,len(main_file)):\n",
    "  processes[i].join()\n",
    "lower_case_file=[q1.get() for i in range(0,len(main_file)) ]\n",
    "processes=[]\n",
    "lock = mp.Lock()\n",
    "q2=mp.Queue()\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  l1=lower_case_file[i]\n",
    "  num = l1[0]\n",
    "  lower_case_file[i] = l1[1:]\n",
    "  p=mp.Process(target=removal_stop_words,args=(lower_case_file[i],stop_words,q2,num,lock))\n",
    "  processes.append(p)\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  processes[i].start()\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  processes[i].join()\n",
    "print(lower_case_file)\n",
    "lower_case_file=[q2.get() for i in range(0,len(lower_case_file))]\n",
    "lower_set_file=[]\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  print(lower_case_file[i])\n",
    "print(len(lower_case_file[5]))\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  list1=lower_case_file[i]\n",
    "  num=list1[0]\n",
    "  list1=list1[1:]\n",
    "  list2=list(set(list1))\n",
    "  list1=[num]+list1\n",
    "  list2=[num]+list2\n",
    "  lower_set_file.append(list2)\n",
    "print(lower_set_file)\n",
    "q3 = mp.Queue()\n",
    "q4 = mp.Queue()\n",
    "processes=[]\n",
    "lock = mp.Lock()\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  l1=lower_case_file[i]\n",
    "  num4 = l1[0]\n",
    "  lower_case_file[i] = l1[1:]\n",
    "  l2 = lower_set_file[i]\n",
    "  num5 = l2[0]\n",
    "  l3=l2[1:]\n",
    "  p=mp.Process(target=find_tf_ntf, args=(lower_case_file[i],l3,q3,q4,num4,num5,lock))\n",
    "  processes.append(p)\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  processes[i].start()\n",
    "for i in range(0,len(lower_case_file)):\n",
    "  processes[i].join()\n",
    "lower_tf_file=[q3.get() for i in range(0,len(lower_case_file))]\n",
    "lower_ntf_file = [q4.get() for i in range(0, len(lower_case_file))]\n",
    "query_list=[]\n",
    "query_tf=[]\n",
    "query_ntf=[]\n",
    "print(lower_tf_file)\n",
    "print(lower_ntf_file)\n",
    "for i in range(0,len(lower_set_file)):\n",
    "  list1=lower_set_file[i]\n",
    "  if(list1[0]==len(lower_set_file)-1):\n",
    "    query_list=list1\n",
    "    break\n",
    "for i in range(0,len(lower_tf_file)):\n",
    "  list1=lower_tf_file[i]\n",
    "  if(list1[0]==len(lower_tf_file)-1):\n",
    "    query_tf=list1\n",
    "    break\n",
    "for i in range(0,len(lower_ntf_file)):\n",
    "  list1=lower_ntf_file[i]\n",
    "  if(list1[0]==len(lower_ntf_file)-1):\n",
    "    query_ntf=list1\n",
    "    break\n",
    "print(query_list)\n",
    "print(query_tf)\n",
    "query_list=query_list[1:]\n",
    "query_ntf = query_ntf[1:]\n",
    "query_tf = query_tf[1:]\n",
    "print(query_list)\n",
    "print(query_ntf)\n",
    "print(query_tf)\n",
    "\n",
    "q6=mp.Queue()\n",
    "q7=mp.Queue()\n",
    "processes=[]\n",
    "lock = mp.Lock()\n",
    "for i in range(len(query_list)):\n",
    "  p=mp.Process(target=idf_inc_query,args=(query_list[i],lower_set_file,q6,q7,lock))\n",
    "  processes.append(p)\n",
    "for i in range(len(query_list)):\n",
    "  processes[i].start()\n",
    "for i in range(len(query_list)):\n",
    "  processes[i].join()\n",
    "query_list=[q6.get() for i in range(len(query_list))]\n",
    "query_idf=[q7.get() for i in range(len(query_list))]\n",
    "print(query_list)\n",
    "print(query_idf)\n",
    "index=0\n",
    "for i in range(0,len(lower_set_file)):\n",
    "  list1=lower_set_file[i]\n",
    "  if(list1[0]==(len(lower_set_file)-1)):\n",
    "    index=i\n",
    "    break\n",
    "del lower_set_file[index]\n",
    "index = 0\n",
    "for i in range(0, len(lower_tf_file)):\n",
    "  list1 = lower_tf_file[i]\n",
    "  if (list1[0] == (len(lower_tf_file) - 1)):\n",
    "      index = i\n",
    "      break\n",
    "del lower_tf_file[index]\n",
    "index = 0\n",
    "for i in range(0, len(lower_ntf_file)):\n",
    "  list1 = lower_ntf_file[i]\n",
    "  if (list1[0] == (len(lower_ntf_file) - 1)):\n",
    "      index = i\n",
    "      break\n",
    "del lower_ntf_file[index]\n",
    "print(lower_set_file)\n",
    "print(lower_tf_file)\n",
    "print(lower_ntf_file)\n",
    "q8 = mp.Queue()\n",
    "q9 = mp.Queue()\n",
    "lock = mp.Lock()\n",
    "processes = []\n",
    "for i in range(len(query_list)):\n",
    "  p = mp.Process(target=idf_exc_query, args=(query_list[i], lower_set_file, q8, q9,lock))\n",
    "  processes.append(p)\n",
    "for i in range(len(query_list)):\n",
    "  processes[i].start()\n",
    "for i in range(len(query_list)):\n",
    "  processes[i].join()\n",
    "query_list = [q8.get() for i in range(len(query_list))]\n",
    "query_idf1 = [q9.get() for i in range(len(query_list))]\n",
    "print(query_list)\n",
    "print(query_idf1)\n",
    "q3 = mp.Queue()\n",
    "processes=[]\n",
    "lock = mp.Lock()\n",
    "for i in range(len(lower_set_file)):\n",
    "  list1=lower_set_file[i]\n",
    "  for j in range(len(lower_ntf_file)):\n",
    "    list2=lower_ntf_file[j]\n",
    "    if(list1[0]==list2[0]):\n",
    "      num=list1[0]\n",
    "      list1=list1[1:]\n",
    "      list2=list2[1:]\n",
    "      p=mp.Process(target=find_query_docs_tf_idf,args=(list1,list2,query_list,q3,num,lock))\n",
    "      processes.append(p)\n",
    "for i in range(0,len(lower_set_file)):\n",
    "  processes[i].start()\n",
    "for i in range(0,len(lower_set_file)):\n",
    "  processes[i].join()\n",
    "query_docs_tf_idf=[q3.get() for i in range(0,len(lower_set_file))]\n",
    "query_tf_mul_idf = []\n",
    "for i in range(0, len(query_idf)):\n",
    "  query_tf_mul_idf.append(query_idf[i] * query_ntf[i])\n",
    "print(query_docs_tf_idf)\n",
    "print(query_tf_mul_idf)\n",
    "q11 = mp.Queue()\n",
    "q12 = mp.Queue()\n",
    "processes = []\n",
    "lock = mp.Lock()\n",
    "for i in range(len(query_docs_tf_idf)):\n",
    "  list1 = query_docs_tf_idf[i]\n",
    "  num10 = list1[0]\n",
    "  list1=list1[1:]\n",
    "  p=mp.Process(target=find_doc_tf_dot,args=(list1,query_idf1,q11,q12,num10,lock))\n",
    "  processes.append(p)\n",
    "for i in range(len(query_docs_tf_idf)):\n",
    "  processes[i].start()\n",
    "for i in range(len(query_docs_tf_idf)):\n",
    "  processes[i].join()\n",
    "docs_tf_mul_idf=[q11.get() for i in range(len(query_docs_tf_idf))]\n",
    "docs_dot_prod=[q12.get() for i in range(len(query_docs_tf_idf))]\n",
    "print(docs_tf_mul_idf)\n",
    "print(docs_dot_prod)\n",
    "print(\"PARALLEL EXECUTION\\n\")\n",
    "print(\"Dot Product of Query\\n\")\n",
    "query_dot_prod = 0\n",
    "for i in range(0, len(query_tf_mul_idf)):\n",
    "  num4 = query_ntf[i] * query_idf[i]\n",
    "  query_dot_prod = query_dot_prod + ((num4) ** 2)\n",
    "query_dot_prod = math.sqrt(query_dot_prod)\n",
    "print(query_dot_prod)\n",
    "\n",
    "print(\"PARALLEL EXECUTION\\n\")\n",
    "q14 = mp.Queue()\n",
    "lock = mp.Lock()\n",
    "processes=[]\n",
    "for i in range(0, len(docs_tf_mul_idf)):\n",
    "  list1=docs_tf_mul_idf[i]\n",
    "  num15 = list1[0]\n",
    "  list1 = list1[1:]\n",
    "  docs_d_p = docs_dot_prod[i][1]\n",
    "  processes.append(p)\n",
    "for i in range(0, len(docs_tf_mul_idf)):\n",
    "  processes[i].start()\n",
    "for i in range(0, len(docs_tf_mul_idf)):\n",
    "  processes[i].join()\n",
    "\n",
    "print(\"PARALLEL EXECUTION\\n\")\n",
    "end = time.perf_counter()\n",
    "print('It took '+str(end-start)+' seconds to execute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
